---
title: "p8105_hw3_yw3236"
output: github_document
author: "Yishan Wang"
date: 2018-10-15
---

# Problem 1

```{r set up library, include = FALSE}
library(tidyverse)
library(dplyr)
```

```{r}
devtools::install_github("p8105/p8105.datasets")
```

```{r}
library(p8105.datasets)
data(brfss_smart2010) 
```

### Clean Data

* Format the data to use appropriate variable names
* Focus on the “Overall Health” topic
* Include only responses from “Excellent” to “Poor”
* Organize responses as a factor taking levels ordered from “Excellent” to “Poor”

```{r}
formated_brfss_smart2010 = brfss_smart2010 %>%
  janitor::clean_names() %>%
  rename(., state = locationabbr, county = locationdesc) %>%
  filter(., topic == "Overall Health") %>%
  filter(., response == "Excellent" | response == "Very good" | response == "Good" | response == "Fair" | response == "Poor") %>%
  mutate(., factor(response, levels = c("Excellent", "Very good", "Good", "Fair", "Poor")))

formated_brfss_smart2010
```

### Question 1

```{r}
formated_brfss_smart2010_2002 = formated_brfss_smart2010 %>%
  filter(., year == "2002") %>%
  select(., year, state, county) %>%
  unique(.) %>%
  group_by(state) %>%
  summarize(n = n())
```

In 2002, CT, FL, NC were observed at 7 locations.

### Question 2

Make a “spaghetti plot” that shows the number of locations in each state from 2002 to 2010.

```{r}
plot_2002_to_2010 = formated_brfss_smart2010 %>%
  select(., year, state) %>%
  group_by(year, state) %>%
  summarize(n_location = n())

ggplot(plot_2002_to_2010, aes(x = year, y = n_location, color = state)) +
  geom_line() +
  labs(x = "Year", y = "Number of Locations")

plot_2002_to_2010
```

There are many lines in the plot, just like "spaghetti". It's kind of difficult to read information from this plot.

### Question 3

Make a table showing, for the years 2002, 2006, and 2010, the mean and standard deviation of the proportion of “Excellent” responses across locations in NY State.

```{r}
formated_brfss_smart2010_excellent_NY = formated_brfss_smart2010 %>%
  filter(., year == "2002" | year == "2006" | year == "2010") %>%
  filter(., state == "NY") %>%
  select(., year, state, county, response, data_value) %>%
  arrange(., year, county) %>%
  filter(., response == "Excellent") %>%
  group_by(year) %>%
  summarise(mean_ny_excellent = mean(data_value), std_ny_excellent = sqrt(var(data_value))) %>%
  knitr::kable(digits = 1) 

formated_brfss_smart2010_excellent_NY
```

From the table, we can see that year 2002 has the highest mean and standard deviation of the proportion of “Excellent” responses across locations in NY State.

### Question 4

* For each year and state, compute the average proportion in each response category. * Make a five-panel plot that shows, for each response category separately, the distribution of these state-level averages over time.

```{r}
response_average_prop = formated_brfss_smart2010 %>%
  select(., year, state, county, response, data_value) %>%
  group_by(year, state, response) %>%
  summarise(average_prop = mean(data_value))

response_average_prop

ggplot(response_average_prop, aes(x = year, y = average_prop)) +
  geom_point() +
  facet_grid(~response)
```

For the **excellent** response, the average proportion of it from 2002 to 2010 is approximatelly between 15% and 30 %. For the **very good** response, the average proportion of it from 2002 to 2010 is approximatelly between 25% and 40%. For the **good** response, the average proportion of it from 2002 to 2010 is approximatelly between 25% and 35%. For the **fair** response, the average proportion of it from 2002 to 2010 is approximatelly between 5% and 15%. For the **poor** response, the average proportion of it from 2002 to 2010 is approximatelly between 0% and 10%.

# Problem 2

```{r}
data(instacart)
```

#### Description of the Dataset

The dataset is about instacart company sale information. The size of the dataset is 1,384,617 x 15. The structure of the dataset is ready for the analysis. The key variables are `order_dow`, `order_hour_of_day`, `product_name`, `aisle`. Each observation includes custmers' information, ordered products' imformation. 

### Question 1

```{r}
n_aisles = nrow(instacart %>%
  select(., aisle) %>%
  unique(.))

n_aisles

most_order_aisles = instacart %>%
  select(., aisle, product_id) %>%
  arrange(., aisle, product_id) %>%
  group_by(aisle) %>%
  summarise(item_num = n()) %>%
  arrange(., item_num) %>%
  tail(5)

most_order_aisles
```

There are 134 aisles there. The most items ordered from fresh vegetables aisle.

### Question 2

Make a plot that shows the number of items ordered in each aisle.

```{r}
order_aisles = instacart %>%
  select(., aisle, product_id) %>%
  arrange(., aisle, product_id) %>%
  group_by(aisle) %>%
  summarise(item_num = n()) %>%
  arrange(., desc(item_num))

ggplot(order_aisles, aes(x = reorder(aisle, -item_num), y = item_num)) +  
  geom_point() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, size = rel(1))) +
  labs(x = "Aisle", y = "Number of Items")

order_aisles
```

Since there are too many aisles in the x axis, I choose to order the `item_num` as decending order and adjust the theme of the graph, such as set the angle is 90. From the graph, we can see that there are few items that has really high number of items. The number of items of the majority of aisles is under 50000. 

### Question 3

Make a table showing the most popular item in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”.

```{r}
popular_aisles = instacart %>%
  select(., aisle, product_id, product_name) %>%
  filter(., aisle == "baking ingredients" | aisle == "dog food care" | aisle == "packaged vegetables fruits") %>%
  arrange(., aisle, product_id) %>%
  group_by(aisle, product_id, product_name) %>%
  summarise(item_num = n())
  
popular_bi_item = popular_aisles %>%
  filter(., aisle == "baking ingredients") %>%
  arrange(., item_num) %>%
  tail(1)

popular_dfc_item = popular_aisles %>%
  filter(., aisle == "dog food care") %>%
  arrange(., item_num) %>%
  tail(1)

popular_pvf_item = popular_aisles %>%
  filter(., aisle == "packaged vegetables fruits") %>%
  arrange(., item_num) %>%
  tail(1)

tibble(
  ailse = c("baking ingredients", "dog food care", "packaged vegetables fruits"),
  popular_item = c(popular_bi_item$product_name, popular_dfc_item$product_name, popular_pvf_item$product_name)
) %>%
  knitr::kable(digits = 1) 
```

By looking at the table, we can find the most poplular items in each aisle mentioned above. We can see that light brown sugar is the most popular ingredients. Dog owners love Snack Sticks Chicken & Rice Recipe Dog Treats. The most popular vegetable is organic baby spinach. From the table, the company can buy more those popular items for sale. 

### Question 4

Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week.

```{r}
mean_hour = instacart %>%
  select(., product_name, order_hour_of_day, order_dow) %>%
  filter(., product_name == "Pink Lady Apples" | product_name == "Coffee Ice Cream") %>%
  group_by(order_dow, product_name) %>%
  summarise(mean_hour_day = mean(order_hour_of_day)) %>%
  rename(day = order_dow) 

table_mean_hour = mean_hour %>%
  mutate(mean_hour_day = round(mean_hour_day, 2)) %>%
  mutate(mean_hour_day = format(mean_hour_day, nsmall = 2)) %>%
  separate(mean_hour_day, into = c("hours", "mins"), sep = "\\.") %>%   
  mutate(mins = round(as.numeric(mins)*0.6)) %>%  
  mutate(average_time_order = paste(hours, ":", mins)) %>% 
  select(day, product_name, average_time_order) %>%   
  spread(key = day, value = average_time_order) %>% 
  knitr::kable(digits = 2, col.names = c("Product", "Sun", "Mon", "Tue", "Wed", "Thu", "Fri", "Sat"),
               caption = "Mean Time of Day Most Listed Items Were Ordered")

table_mean_hour
```

From the table, we can see coffee ice cream is usually bought after 12AM and pink laby apples are usually bought before 12AM.

# Problem 3

```{r}
data(ny_noaa)
```

#### Description of the Dataset

The size of the dataset is 2,595,176 x 7. I would say all of the variables in the dataset are key variables because different problems need different variables. The structure of this data is ready for the analysis, except for seperating date variable. Each variable of this dataset have lots of missing values. Those missing data might couse not accurate result.

### Question 1

Separate variables for year, month, and day.

```{r}
formated_ny_noaa = ny_noaa %>% 
  ungroup() %>%
  mutate(year = format(date, "%Y"), month = format(date, "%m"), day = format(date, "%d")) %>%
  mutate(prcp = prcp/10, tmax = as.numeric(tmax)/10, tmin = as.numeric(tmin)/10) %>%
  select(., id, year, month, day, prcp, snow, snwd, tmax, tmin) 

formated_ny_noaa

common_snwd = formated_ny_noaa %>%
  select(., snwd) %>%
  filter(., !is.na(snwd)) %>%
  group_by(snwd) %>%
  summarise(n = n()) %>%
  arrange(., n) %>%
  tail(1)

common_snwd
```

For snowfall, the most commonly observed values are 0. The reason is becasue snow only in winter time and winter time is relatively short comparing a whole year.

### Question 2

Make a two-panel plot showing the average max temperature in January and in July in each station across years. 

```{r}
jan_jul_temp = formated_ny_noaa %>%
  select(., id, year, month, day, tmax) %>%
  filter(., !is.na(tmax)) %>%
  filter(., month == "01" | month == "07") %>%
  group_by(year, month, id) %>%
  summarise(average_max_temp = mean(tmax))

ggplot(jan_jul_temp, aes(x = year, y = average_max_temp)) +
  geom_point() +
  facet_grid(~month)

ggplot(jan_jul_temp, aes(x = year, y = average_max_temp)) +
  geom_boxplot() +
  facet_grid(~month) 
```

There is an observable structure. The average max temperature in Jan is lower than the average max temperature in Jul. There are outiers. Those outilers are easily to be identified from boxplot.